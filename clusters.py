import kneed
import sklearn
import numpy as np
from kneed import KneeLocator
from sklearn.cluster import KMeans
from collections import Counter
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
import pandas as pd
import matplotlib.pyplot as plt
from collections import defaultdict


# Create function that runs kmeans with a range of clusters with the data, test the accuracy (inertia) of different cluster numbers representing the raw data using the elbow method, return the optimal number of clusters

def K(data, max_k = 11, plot = False):

    inertias = []
    for i in range(1,max_k):
        kmeans = KMeans(n_clusters=i)
        kmeans.fit(data)
        inertias.append(kmeans.inertia_)

    kl = KneeLocator(range(1, max_k), inertias, curve="convex", direction="decreasing")
    if plot :
        kl.plot_knee()
    return kl.elbow


# normalize data
def normalize(data) :
	'''Normalize data so each column has a mean of 0 and a standard deviation of 1'''
	return (data - np.mean(data, axis=0)) / np.std(data, axis=0)

# Create function that plots kmeans data based on the data and elbow method
def plotKMeans(data, labels1, labels2 = None): 
    """"
    input data,
    returns plot of data color-coded by labels1; If labels2 is specified, we use 'x' to indicate a difference between label values (e.g., incorrect classification)
    
    """

    date = np.array(data)
    x, y = date.T

    if np.any(labels2 == None):
       plt.scatter(x,y,c = labels1)
       return None

    assignedDict = labelClusters(labels1, labels2)
    assigned = [assignedDict[l] for l in labels1]
    correct = np.array(assigned) == np.array(labels2)
    wrong = np.logical_not(correct)

    fig, ax = plt.subplots()
    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
    i = 0
    for s in set(labels1) :
        index = np.logical_and(correct, labels1 == s)
        ax.scatter(x[index], y[index], c=colors[i], label = assignedDict[s])
    
        index = np.logical_and(wrong, labels1 == s)
        ax.scatter(x[index], y[index], c=colors[i], label = '_no_legend_', marker = 'x')
    
        i+= 1
    
    handles, labels = plt.gca().get_legend_handles_labels()
    order = np.argsort(labels)
    ax.legend([handles[idx] for idx in order],[labels[idx] for idx in order]) 
    plt.show()


def labelClusters(klabels, labels, print_df = False):
    """
    enter the arbitrary labels generated by kmeans and the true labels. This function returns a disctionary telling you
    which kmeans cluster is most associated with which true label.
    """
    # create default dictionary with default an empty list
    # the key is the klabel and the value is the list of values for that label
    counters = defaultdict(list)

    # for each klabel and label
    for k, l in zip(klabels, labels) :    
        # add the label to the dictionary
        counters[k] += [l]
        
    # count the number of values for each klabel
    for k in counters :
        counters[k] = Counter(counters[k])
    counters
    
    # create a dictionary of clusterLabels based on the most common value for each klabel
    clusterLabels = {}
    for k in counters:
        c = counters[k]
        clusterLabels[k] = c.most_common(1)[0][0]
        
    return clusterLabels


def getClusters(data, labels):
    """"
    input: data, labels
    returns tuple: cluster labels, number of clusters
    
    """
    
    y = K(data)
    gSList = set(labels)
    glist = []
    for x in gSList:
        glist.append(x)
    glist.append(y)
    v = tuple(glist)
    return v

def randIndex(klabels, labels):
    """"
    input: klabels, labels
    
    uses true cluster labels to measure accuracy of kmeans on data. rand index takes compares 
    how well the predicted clusters and the true clusters line up. 
    
    return rand index
    """
    score = sklearn.metrics.rand_score(klabels, labels)
    
    return score 

def use_kmeans(data, labels):
    """
    enter data and labels as np arrays. this function preforms kmeans on the data and then compares the arbitrary 
    cluster labels to the true labels, giving the rand index. This function returns the kmeans object, the rand index 
    and the number of clusters as a tuple in that order.
    """
    k = K(data)
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(data)
    
    return kmeans, randIndex(kmeans.labels_, labels), k

def importData(data, labels):
    """
    enter name of data file in csv format, enter name of labels file in csv format. Make sure both files are local so they 
    can be directly called. This function spits out a tuple with a 2D numpy array of the data first and a 1D array of the 
    labels second.
    """
    data = pd.read_csv('iris_features.csv', 
                    header=0, sep='\t+', engine='python')
    labelz = pd.read_csv('iris_labels.csv', header = 0)
    labels = np.array(labelz.iloc[:, 0])

    dlist = []
    for x in range(len(data)):
        dlist.append(list(data.loc[x]))
    darr = np.array(dlist)
    
    return darr, labels